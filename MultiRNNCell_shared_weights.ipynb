{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiRNNCell with shared weights\n",
    "A MultiRNNCell object that stacks several LSTM cell with weight sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # 0.12.1\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "# import math\n",
    "\n",
    "# These can be simplified with tf.<fn> instead\n",
    "# from tensorflow.python.ops.math_ops import sigmoid\n",
    "# from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "# from tensorflow.python.framework import ops\n",
    "# from tensorflow.python.ops import array_ops\n",
    "# from tensorflow.python.ops import clip_ops\n",
    "# from tensorflow.python.ops import nn_ops\n",
    "# from tensorflow.python.ops import math_ops\n",
    "# from tensorflow.python.ops import init_ops\n",
    "# from tensorflow.python.ops import variable_scope as vs\n",
    "\n",
    "# from tensorflow.python.util import nest  # some Tensorflow magic\n",
    "\n",
    "# Some jupyter magic\n",
    "# automatic debugging\n",
    "# %pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Heavily based on original tf.nn.rnn_cell_LSTMCell()\n",
    "# with modifications to variable_scope to allow for Tensorflow \n",
    "# weight sharing.\n",
    "\n",
    "\n",
    "# LSTMCell helper functions\n",
    "def _get_concat_variable(name, shape, dtype, num_shards):\n",
    "    \"\"\"Get a sharded variable concatenated into one tensor.\"\"\"\n",
    "    sharded_variable = _get_sharded_variable(name, shape, dtype, num_shards)\n",
    "    if len(sharded_variable) == 1:\n",
    "        return sharded_variable[0]\n",
    "\n",
    "    concat_name = name + \"/concat\"\n",
    "    concat_full_name = vs.get_variable_scope().name + \"/\" + concat_name + \":0\"\n",
    "    for value in ops.get_collection(ops.GraphKeys.CONCATENATED_VARIABLES):\n",
    "        if value.name == concat_full_name:\n",
    "            return value\n",
    "\n",
    "    concat_variable = array_ops.concat(0, sharded_variable, name=concat_name)\n",
    "    ops.add_to_collection(ops.GraphKeys.CONCATENATED_VARIABLES,\n",
    "                          concat_variable)\n",
    "    return concat_variable\n",
    "\n",
    "\n",
    "# LSTMCell helper functions\n",
    "def _get_sharded_variable(name, shape, dtype, num_shards):\n",
    "    \"\"\"Get a list of sharded variables with the given dtype.\"\"\"\n",
    "    if num_shards > shape[0]:\n",
    "        raise ValueError(\"Too many shards: shape=%s, num_shards=%d\" %\n",
    "                       (shape, num_shards))\n",
    "    unit_shard_size = int(math.floor(shape[0] / num_shards))\n",
    "    remaining_rows = shape[0] - unit_shard_size * num_shards\n",
    "\n",
    "    shards = []\n",
    "    for i in range(num_shards):\n",
    "        current_size = unit_shard_size\n",
    "        if i < remaining_rows:\n",
    "            current_size += 1\n",
    "        shards.append(vs.get_variable(name + \"_%d\" % i, [current_size] + shape[1:],\n",
    "                                      dtype=dtype))\n",
    "    return shards\n",
    "\n",
    "\n",
    "class LSTMCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"Long short-term memory unit (LSTM) recurrent network cell.\n",
    "\n",
    "    The default non-peephole implementation is based on:\n",
    "\n",
    "        http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "\n",
    "    S. Hochreiter and J. Schmidhuber.\n",
    "    \"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997.\n",
    "\n",
    "    The peephole implementation is based on:\n",
    "\n",
    "        https://research.google.com/pubs/archive/43905.pdf\n",
    "\n",
    "    Hasim Sak, Andrew Senior, and Francoise Beaufays.\n",
    "    \"Long short-term memory recurrent neural network architectures for\n",
    "     large scale acoustic modeling.\" INTERSPEECH, 2014.\n",
    "\n",
    "    The class uses optional peep-hole connections, optional cell clipping, and\n",
    "    an optional projection layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_units, input_size=None,\n",
    "                 use_peepholes=False, cell_clip=None,\n",
    "                 initializer=None, num_proj=None, proj_clip=None,\n",
    "                 num_unit_shards=1, num_proj_shards=1,\n",
    "                 forget_bias=1.0, state_is_tuple=True,\n",
    "                 activation=tanh):\n",
    "        \"\"\"Initialize the parameters for an LSTM cell.\n",
    "\n",
    "        Args:\n",
    "            num_units: int, The number of units in the LSTM cell\n",
    "            input_size: Deprecated and unused.\n",
    "            use_peepholes: bool, set True to enable diagonal/peephole connections.\n",
    "            cell_clip: (optional) A float value, if provided the cell state is clipped\n",
    "                by this value prior to the cell output activation.\n",
    "            initializer: (optional) The initializer to use for the weight and\n",
    "                projection matrices.\n",
    "            num_proj: (optional) int, The output dimensionality for the projection\n",
    "                matrices.  If None, no projection is performed.\n",
    "            proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n",
    "            provided, then the projected values are clipped elementwise to within\n",
    "            `[-proj_clip, proj_clip]`.\n",
    "            num_unit_shards: How to split the weight matrix.  If >1, the weight\n",
    "                matrix is stored across num_unit_shards.\n",
    "            num_proj_shards: How to split the projection matrix.  If >1, the\n",
    "                projection matrix is stored across num_proj_shards.\n",
    "            forget_bias: Biases of the forget gate are initialized by default to 1\n",
    "                in order to reduce the scale of forgetting at the beginning of\n",
    "                the training.\n",
    "            state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "                the `c_state` and `m_state`.  If False, they are concatenated\n",
    "                along the column axis.  This latter behavior will soon be deprecated.\n",
    "            activation: Activation function of the inner states.\n",
    "        \"\"\"\n",
    "        if not state_is_tuple:\n",
    "            tf.logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "                            \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "        if input_size is not None:\n",
    "            tf.logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "        self._num_units = num_units\n",
    "        self._use_peepholes = use_peepholes\n",
    "        self._cell_clip = cell_clip\n",
    "        self._initializer = initializer\n",
    "        self._num_proj = num_proj\n",
    "        self._proj_clip = proj_clip\n",
    "        self._num_unit_shards = num_unit_shards\n",
    "        self._num_proj_shards = num_proj_shards\n",
    "        self._forget_bias = forget_bias\n",
    "        self._state_is_tuple = state_is_tuple\n",
    "        self._activation = activation\n",
    "\n",
    "        if num_proj:\n",
    "            self._state_size = (\n",
    "                tf.nn.rnn_cell.LSTMStateTuple(num_units, num_proj)\n",
    "                if state_is_tuple else num_units + num_proj)\n",
    "            self._output_size = num_proj\n",
    "        else:\n",
    "            self._state_size = (\n",
    "                tf.nn.rnn_cell.LSTMStateTuple(num_units, num_units)\n",
    "                if state_is_tuple else 2 * num_units)\n",
    "            self._output_size = num_units\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run one step of LSTM.\n",
    "\n",
    "        Args:\n",
    "            inputs: input Tensor, 2D, batch x num_units.\n",
    "            state: if `state_is_tuple` is False, this must be a state Tensor,\n",
    "                `2-D, batch x state_size`.  If `state_is_tuple` is True, this must be a\n",
    "                tuple of state Tensors, both `2-D`, with column sizes `c_state` and\n",
    "                `m_state`.\n",
    "        scope: VariableScope for the created subgraph; defaults to \"LSTMCell\".\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "\n",
    "            - A `2-D, [batch x output_dim]`, Tensor representing the output of the\n",
    "                LSTM after reading `inputs` when previous state was `state`.\n",
    "                Here output_dim is:\n",
    "                    num_proj if num_proj was set,\n",
    "                    num_units otherwise.\n",
    "            - Tensor(s) representing the new state of LSTM after reading `inputs` when\n",
    "                the previous state was `state`.  Same type and shape(s) as `state`.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input size cannot be inferred from inputs via\n",
    "                static shape inference.\n",
    "        \"\"\"\n",
    "        num_proj = self._num_units if self._num_proj is None else self._num_proj\n",
    "\n",
    "        if self._state_is_tuple:\n",
    "            (c_prev, m_prev) = state\n",
    "        else:\n",
    "            c_prev = array_ops.slice(state, [0, 0], [-1, self._num_units])\n",
    "            m_prev = array_ops.slice(state, [0, self._num_units], [-1, num_proj])\n",
    "\n",
    "        dtype = inputs.dtype\n",
    "        input_size = inputs.get_shape().with_rank(2)[1]\n",
    "        if input_size.value is None:\n",
    "            raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\n",
    "        with vs.variable_scope(scope or type(self).__name__,\n",
    "                               initializer=self._initializer):  # \"LSTMCell\"\n",
    "            concat_w = _get_concat_variable(\n",
    "                \"W\", [input_size.value + num_proj, 4 * self._num_units],\n",
    "                dtype, self._num_unit_shards)\n",
    "\n",
    "            b = vs.get_variable(\n",
    "                \"B\", shape=[4 * self._num_units],\n",
    "                initializer=init_ops.zeros_initializer, dtype=dtype)\n",
    "\n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            cell_inputs = array_ops.concat(1, [inputs, m_prev])\n",
    "            lstm_matrix = nn_ops.bias_add(math_ops.matmul(cell_inputs, concat_w), b)\n",
    "            i, j, f, o = array_ops.split(1, 4, lstm_matrix)\n",
    "\n",
    "            # Diagonal connections\n",
    "            if self._use_peepholes:\n",
    "                w_f_diag = vs.get_variable(\n",
    "                    \"W_F_diag\", shape=[self._num_units], dtype=dtype)\n",
    "                w_i_diag = vs.get_variable(\n",
    "                    \"W_I_diag\", shape=[self._num_units], dtype=dtype)\n",
    "                w_o_diag = vs.get_variable(\n",
    "                    \"W_O_diag\", shape=[self._num_units], dtype=dtype)\n",
    "\n",
    "            if self._use_peepholes:\n",
    "                c = (sigmoid(f + self._forget_bias + w_f_diag * c_prev) * c_prev +\n",
    "                     sigmoid(i + w_i_diag * c_prev) * self._activation(j))\n",
    "            else:\n",
    "                c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) * \n",
    "                     self._activation(j))\n",
    "\n",
    "            if self._cell_clip is not None:\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)\n",
    "                # pylint: enable=invalid-unary-operand-type\n",
    "\n",
    "            if self._use_peepholes:\n",
    "                m = sigmoid(o + w_o_diag * c) * self._activation(c)\n",
    "            else:\n",
    "                m = sigmoid(o) * self._activation(c)\n",
    "\n",
    "            if self._num_proj is not None:\n",
    "                concat_w_proj = _get_concat_variable(\n",
    "                    \"W_P\", [self._num_units, self._num_proj],\n",
    "                        dtype, self._num_proj_shards)\n",
    "\n",
    "                m = math_ops.matmul(m, concat_w_proj)\n",
    "                if self._proj_clip is not None:\n",
    "                    # pylint: disable=invalid-unary-operand-type\n",
    "                    m = clip_ops.clip_by_value(m, -self._proj_clip, self._proj_clip)\n",
    "                    # pylint: enable=invalid-unary-operand-type\n",
    "\n",
    "        print('DEBUG')\n",
    "        new_state = (tf.nn.rnn_cell.LSTMStateTuple(c, m) if self._state_is_tuple\n",
    "                 else array_ops.concat(1, [c, m]))\n",
    "        return m, new_state\n",
    "\n",
    "'''\n",
    "Custom LSTMCell\n",
    "\n",
    "Based on http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Modified tensorflow.nn.rnn_cell.LSTMCell to remove variable scopes.\n",
    "\n",
    "This cell allows us to customise variable scope which in turn\n",
    "lets us share variables between cells. It is otherwise identical.\n",
    "''' \n",
    "class BasicLSTMCell_shared_weights(tf.nn.rnn_cell.RNNCell):\n",
    "    def __init__(self, num_units, forget_bias=1.0, input_size=None,\n",
    "                 activation=tanh):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.nn.rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "    \n",
    "        c, h = state\n",
    "        \n",
    "        # concat\n",
    "        # concat = _linear([inputs, h], 4 * self._num_units, True)\n",
    "        \n",
    "        args = [inputs, h]\n",
    "        \n",
    "        total_arg_size = 0\n",
    "        shapes = [a.get_shape().as_list() for a in args]\n",
    "        for shape in shapes:\n",
    "            total_arg_size += shape[1]\n",
    "        \n",
    "        output_size = 4 * self._num_units\n",
    "        \n",
    "        # shared\n",
    "        with tf.variable_scope('LSTMCell_shared_weights'):\n",
    "            matrix = tf.get_variable('W', [total_arg_size, output_size])\n",
    "            bias_term = tf.get_variable('b', [output_size],\n",
    "                initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "        res = tf.matmul(tf.concat(1, args), matrix) + bias_term\n",
    "        \n",
    "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "        i, j, f, o = tf.split(1, 4, res)\n",
    "        \n",
    "        new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) * \n",
    "                 self._activation(j))\n",
    "        new_h = self._activation(new_c) * sigmoid(o)\n",
    "        \n",
    "        new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "        return new_h, new_state\n",
    "\n",
    "'''\n",
    "Custom MultiRNNCell wrapper\n",
    "\n",
    "Exactly the Same as tf.nn.rnn_cell.MultiRNNCell but without the \n",
    "name scoping. The original wrapper adds /cell# to the name scope,\n",
    "which would have created new variables instead of reusing exising\n",
    "ones.\n",
    "\n",
    "This class enables you to stack LSTMCells ontop of each other with\n",
    "the same input. The multicell simply passes the input and output to\n",
    "the cells in the stack.\n",
    "'''\n",
    "class MultiRNNCell_shared_weights(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"RNN cell composed sequentially of multiple simple cells.\"\"\"\n",
    "    def __init__(self, cells):\n",
    "        self._cells = cells\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tuple(cell.state_size for cell in self._cells)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._cells[-1].output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n",
    "        cur_state_pos = 0\n",
    "        cur_inp = inputs\n",
    "        new_states = []\n",
    "        for i, cell in enumerate(self._cells):\n",
    "            cur_state = state[i]\n",
    "            cur_inp, new_state = cell(cur_inp, cur_state)\n",
    "            new_states.append(new_state)\n",
    "        new_states = tuple(new_states)\n",
    "        return cur_inp, new_states\n",
    "\n",
    "# From tf.nn.rnn_cell._linear()\n",
    "# Original authors: tensorflow\n",
    "# This could be expanded to n-dimensions later\n",
    "# def _linear(args, output_size, bias, bias_start=0.0, scope=None):\n",
    "#     \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "\n",
    "#     Args:\n",
    "#         args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "#         output_size: int, second dimension of W[i].\n",
    "#         bias: boolean, whether to add a bias term or not.\n",
    "#         bias_start: starting value to initialize the bias; 0 by default.\n",
    "#         scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "\n",
    "#     Returns:\n",
    "#         A 2D Tensor with shape [batch x output_size] equal to\n",
    "#         sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "\n",
    "#     Raises:\n",
    "#         ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "#     \"\"\"\n",
    "#     if args is None or (nest.is_sequence(args) and not args):\n",
    "#         raise ValueError(\"`args` must be specified\")\n",
    "#     if not nest.is_sequence(args):\n",
    "#         args = [args]\n",
    "\n",
    "#     # Calculate the total size of arguments on dimension 1.\n",
    "#     total_arg_size = 0\n",
    "#     shapes = [a.get_shape().as_list() for a in args]\n",
    "#     for shape in shapes:\n",
    "#     if len(shape) != 2:\n",
    "#         raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "#     if not shape[1]:\n",
    "#         raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "#     else:\n",
    "#         total_arg_size += shape[1]\n",
    "\n",
    "#         dtype = [a.dtype for a in args][0]\n",
    "\n",
    "#         # Now the computation.\n",
    "#         with tf.variable_scope(scope or \"Linear\"):\n",
    "#             matrix = tf.get_variable(\n",
    "#                 \"Matrix\", [total_arg_size, output_size], dtype=dtype)\n",
    "#         if len(args) == 1:\n",
    "#             res = tf.matmul(args[0], matrix)\n",
    "#         else:\n",
    "#             res = tf.matmul(tf.concat(1, args), matrix)\n",
    "#         if not bias:\n",
    "#             return res\n",
    "#         bias_term = tf.get_variable(\n",
    "#             \"Bias\", [output_size],\n",
    "#             dtype=dtype,\n",
    "#             initializer=tf.constant_initializer(\n",
    "#                 bias_start, dtype=dtype))\n",
    "#         return res + bias_term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM equations\n",
    "$i = \\sigma(x_tU^i + s_{t-1}W^i + b^i) \\\\\n",
    "f = \\sigma(x_tU^f + s_{t-1}W^f + b^f) \\\\\n",
    "o = \\sigma(x_tU^o + s_{t-1}W^o + b^o) \\\\\n",
    "g = \\tanh(x_tU^g + s_{t-1}W^g + b^g) \\\\\n",
    "c_t = c_{t-1} \\odot f + g \\odot i \\\\\n",
    "s_t = \\tanh(c_t) \\odot o\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix and vector dimensions\n",
    "* $U \\text{ has dimensions } n \\times m$ (hidden to input)\n",
    "* $W \\text{ has dimensions } n \\times n$ (hidden to hidden)\n",
    "* One set of matrixes for each of the three gates {$i, f, o$}\n",
    "* One extra set for the cell state\n",
    "* We also use an optional bias of dimension $n$ for each gate and the state\n",
    "\n",
    "$4(n \\times m) + 4(n^2) + 4n\\\\\n",
    "4n \\times 4m + 4n^2 + 4n\\\\\n",
    "4(nm + n^2 + n)$\n",
    "\n",
    "$n = 32\\\\\n",
    "m = 8$\n",
    "\n",
    "Total parameters: $4(32\\times8 + 32^2 + 32) = 5248$\n",
    "\n",
    "$\\displaystyle \\\\\n",
    "x_t \\in \\mathbb{R}^8 \\\\\n",
    "o_t \\in \\mathbb{R}^8 \\\\\n",
    "s_t \\in \\mathbb{R}^{32} \\\\\n",
    "U^i \\in \\mathbb{R}^{32 \\times 8} \\\\\n",
    "U^f \\in \\mathbb{R}^{32 \\times 8} \\\\\n",
    "U^o \\in \\mathbb{R}^{32 \\times 8} \\\\\n",
    "U^g \\in \\mathbb{R}^{32 \\times 8} \\\\\n",
    "W^i \\in \\mathbb{R}^{32 \\times 32} \\\\\n",
    "W^f \\in \\mathbb{R}^{32 \\times 32} \\\\\n",
    "W^o \\in \\mathbb{R}^{32 \\times 32} \\\\\n",
    "W^g \\in \\mathbb{R}^{32 \\times 32} \\\\\n",
    "b^i \\in \\mathbb{R}^{32} \\\\\n",
    "b^f \\in \\mathbb{R}^{32} \\\\\n",
    "b^o \\in \\mathbb{R}^{32} \\\\\n",
    "b^g \\in \\mathbb{R}^{32}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    '''\n",
    "    Convenience function to reset graph at each run.\n",
    "    If the graph is left open Tensorflow automatically appends new\n",
    "    graph functions making things slow.'''\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()  # Close any open session\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_multicell_lstm_graph(\n",
    "    state_size=16,  #  c + h, use num_units * 2 instead\n",
    "    num_units=32,\n",
    "    batch_size=8,\n",
    "    num_steps=8,\n",
    "    num_classes=8,\n",
    "    num_cells=1,\n",
    "    learning_rate=1e-4):\n",
    "    '''\n",
    "    This function builds a dynamic rnn graph with multiple LSTM cells.\n",
    "    The LSTM cells share weights between them though the modified\n",
    "    MultiRNNCell wrapper.\n",
    "    \n",
    "    Vanilla RNN equations:\n",
    "    st = tanh(Uxt + Wst-1)\n",
    "    ot softmax(Vst)\n",
    "    \n",
    "    H = 8 (num_units)\n",
    "    C = 2 (grid_size)\n",
    "    \n",
    "    xt = 2 (grid_size)\n",
    "    ot = 2\n",
    "    st = 8 (num_units)\n",
    "    U = 8 x 2\n",
    "    V = 2 x 8\n",
    "    W = 8 x 8\n",
    "    \n",
    "    '''\n",
    "    # reset graph for testing purposes\n",
    "    reset_graph()\n",
    "    \n",
    "    # Get dataset\n",
    "    Dataset = build_1d_dataset(n=8, num_samples=1000)\n",
    "    \n",
    "    # Create placeholder for minibatch\n",
    "    # (num_samples, timesteps, depth)\n",
    "    x = tf.placeholder(tf.float32, [None, num_steps, 1])\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    # shape (batch_size, num_steps, state_size)\n",
    "#     rnn_inputs = tf.nn.embedding_lookup(tf.random_normal([num_classes, state_size]), x)\n",
    "\n",
    "    # Create variables\n",
    "#     with tf.variable_scope('RNN') as scope:  # Save scope in variable\n",
    "#         # For LSTM use [2n x 4n] weights for all 4 gates to allow us\n",
    "#         # to multiply them all at once, saving computation.\n",
    "#         W = tf.get_variable('LSTMCell/W_0',\n",
    "#                             [2 * num_units, 4 * num_units])\n",
    "#         # Similarly, we have [4n] biases\n",
    "#         b = tf.get_variable('LSTMCell/B', [4 * num_units],\n",
    "#                             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # Mark scope to reuse from now on. ie: any variable created using\n",
    "    # this scope will reuse variables if they are found with get_variable().\n",
    "    # To create new variables set scope reuse=False or call this function\n",
    "    # again.\n",
    "#     scope.reuse_variables()\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "    \n",
    "    # Use this to stack several cells in a tower\n",
    "#     cell = MultiRNNCell_shared_weights([cell] * num_cells)  # custom wrapper\n",
    "    \n",
    "    # Cells are unrolled in tf.nn.dynamic_rnn and weights are shared between them\n",
    "#     rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32, scope=scope)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('softmax'):  # Save scope in variable\n",
    "        W = tf.get_variable('W', [num_units, num_units])\n",
    "        b = tf.get_variable('b', [num_units], initializer=tf.constant_initializer(0.0))\n",
    "#     logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "    \n",
    "    # flatten to apply weights to all timesteps\n",
    "    rnn_output = tf.reshape(rnn_outputs, [-1, num_units])\n",
    "    prediction = tf.nn.softmax(tf.matmul(rnn_output, W) + b)\n",
    "    prediction = tf.reshape(prediction, [-1, num_steps, 1])\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )\n",
    "    \n",
    "# build_multicell_lstm_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs,\n",
    "                 num_steps=200,\n",
    "                 batch_size=8,\n",
    "                 verbose=True,\n",
    "                 save=False):\n",
    "    tf.set_random_seed(42)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for i, epoch in enumerate(gen_epochs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN/LSTMCell/W_0:0, shape=(33, 128), params=4224\n",
      "RNN/LSTMCell/B:0, shape=(128,), params=128\n",
      "total_parameters: 4352\n",
      "2 variables\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Build graph and test if we only have two variables\n",
    "'''\n",
    "build_multicell_lstm_graph()\n",
    "\n",
    "def count_parameters_and_variables():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        print('{}, shape={}, params={}'.format(variable.name, shape, variable_parameters))\n",
    "        total_parameters += variable_parameters\n",
    "    print('total_parameters:', total_parameters)\n",
    "    \n",
    "    g = tf.get_default_graph()\n",
    "    vars = g.get_collection('variables')\n",
    "    print(len(vars), 'variables')\n",
    "    \n",
    "    return len(vars), total_parameters\n",
    "\n",
    "assert count_parameters_and_variables()[0] == 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np \n",
    "# from collections import namedtuple\n",
    "\n",
    "def generate_samples(num_samples=1000):\n",
    "    pass\n",
    "\n",
    "def build_1d_dataset(\n",
    "    n_length=8,  # length of the grid\n",
    "    num_samples=100000,\n",
    "    k_value=2,  # number of colours k=2: binary\n",
    "    train_split=0.8,  # fraction of full dataset for training\n",
    "    valid_split=0.5,  # fraction of test set for validation\n",
    "#     random_distribution=True,\n",
    "    verbose=False):\n",
    "    '''build 1d board training dataset of num_samples with n length \n",
    "    for machine learning.\n",
    "    \n",
    "    The boards are randomly distributed binary \"stones\" (1 or 0)\n",
    "    \n",
    "    inputs:\n",
    "        n: length of the board (1d)\n",
    "        num_samples: how many samples\n",
    "        k_value: number of colors of the grid default k=2 is binary\n",
    "        train_split: \n",
    "        valid_split: \n",
    "        verbose: show statistics\n",
    "        \n",
    "    returns:\n",
    "        A named tuple 'Dataset' with train, valid and test datasets\n",
    "            of (data, labels) length\n",
    "    '''\n",
    "    \n",
    "    # Generate random num_samples with n length in shape=(num_samples, n)\n",
    "    # It generates the random distribution from 0 to k_value\n",
    "    data = np.random.randint(0, k_value, size=[num_samples, n_length])\n",
    "    \n",
    "    # Generate labels from data\n",
    "    # The arbitrary problem for the machine is to find the\n",
    "    # connection length from left to right.\n",
    "    labels = np.zeros(num_samples, dtype=int)\n",
    "    for i, board in enumerate(data):\n",
    "\n",
    "#         if np.sum(board, axis=0) == n:  # quickly get fully connected boards\n",
    "#             labels[i] = 1\n",
    "#         else:\n",
    "#             labels[i] = 0\n",
    "\n",
    "        # Stepwise look for 1's per grid\n",
    "        connection_length = 0\n",
    "        for j, grid in enumerate(board):\n",
    "            if grid == 1:\n",
    "                connection_length += 1 \n",
    "            else:\n",
    "                break  # stop looking to save some computation\n",
    "        labels[i] = connection_length\n",
    "    \n",
    "    # Create dataset named tuple\n",
    "    Dataset = namedtuple('Dataset', ['train', 'valid', 'test'])\n",
    "    \n",
    "    # Split dataset\n",
    "    num_train = int(num_samples * train_split)\n",
    "    num_valid = int((num_samples - num_train) * valid_split)\n",
    "    \n",
    "    Dataset.train = (data[:num_train], labels[:num_train])\n",
    "    Dataset.valid = (data[:num_valid], labels[:num_valid])\n",
    "    Dataset.test = (data[:num_valid], labels[:num_valid])\n",
    "    \n",
    "    return Dataset\n",
    "\n",
    "# %time build_1d_dataset();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
