{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiRNNCell with shared weights\n",
    "A MultiRNNCell object that stacks several LSTM cell with weight sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # 0.12.1\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "from tensorflow.python.ops import variable_scope as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    '''Convenience function to reset graph'''\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Custom MultiRNNCell wrapper\n",
    "\n",
    "Exactly the Same as tf.nn.rnn_cell.MultiRNNCell but without the \n",
    "name scoping. The original wrapper adds /cell# to the name scope,\n",
    "which would have created new variables instead of reusing exising\n",
    "ones.\n",
    "'''\n",
    "class MultiRNNCell_shared_weights(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"RNN cell composed sequentially of multiple simple cells.\"\"\"\n",
    "    def __init__(self, cells):\n",
    "        self._cells = cells\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tuple(cell.state_size for cell in self._cells)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._cells[-1].output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n",
    "        cur_state_pos = 0\n",
    "        cur_inp = inputs\n",
    "        new_states = []\n",
    "        for i, cell in enumerate(self._cells):\n",
    "            cur_state = state[i]\n",
    "            cur_inp, new_state = cell(cur_inp, cur_state)\n",
    "            new_states.append(new_state)\n",
    "        new_states = tuple(new_states)\n",
    "        return cur_inp, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_multicell_lstm_graph(\n",
    "    state_size=8,\n",
    "    batch_size=8,\n",
    "    num_steps=2,\n",
    "    num_classes=2,\n",
    "    num_cells=2):\n",
    "    '''\n",
    "    This function builds a dynamic rnn graph with multiple LSTM cells \n",
    "    stacked ontop of eachother.\n",
    "    The LSTM cells share weights between them though the modified\n",
    "    MultiRNNCell wrapper.\n",
    "    '''\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    \n",
    "    # shape (batch_size, num_steps, state_size)\n",
    "    rnn_inputs = tf.nn.embedding_lookup(tf.random_normal([num_classes, state_size]), x)\n",
    "\n",
    "    # Create variables\n",
    "    # using the same variable names generated in BasicLSTMCell\n",
    "    with tf.variable_scope('RNN') as scope:  # hack\n",
    "        W = vs.get_variable('BasicLSTMCell/Linear/Matrix',\n",
    "                            [state_size + state_size, state_size * 4])\n",
    "        b = vs.get_variable('BasicLSTMCell/Linear/Bias', [state_size * 4],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "    scope.reuse_variables()  # mark scope to reuse from now on\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "    cell = MultiRNNCell_shared_weights([cell] * num_cells)  # custom wrapper\n",
    "    \n",
    "    # Cells are unrolled in tf.nn.dynamic_rnn and weights are shared between them\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32, scope=scope)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN/BasicLSTMCell/Linear/Matrix:0, shape=(16, 32), params=512\n",
      "RNN/BasicLSTMCell/Linear/Bias:0, shape=(32,), params=32\n",
      "total_parameters: 544\n",
      "2 variables\n"
     ]
    }
   ],
   "source": [
    "build_multicell_lstm_graph()\n",
    "\n",
    "def count_parameters_and_variables():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        print('{}, shape={}, params={}'.format(variable.name, shape, variable_parameters))\n",
    "        total_parameters += variable_parameters\n",
    "    print('total_parameters:', total_parameters)\n",
    "    \n",
    "    g = tf.get_default_graph()\n",
    "    vars = g.get_collection('variables')\n",
    "    print(len(vars), 'variables')\n",
    "    \n",
    "    return len(vars), total_parameters\n",
    "\n",
    "assert count_parameters_and_variables()[0] == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
