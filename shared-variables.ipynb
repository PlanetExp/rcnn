{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker usage\n",
    "Written for Tensorflow 12.1 using python 3.5\n",
    "\n",
    "### Run Jupyter using Docker\n",
    "```bash\n",
    "docker run --rm -it -p 8888:8888 -v $(pwd):/notebooks --name=tf gcr.io/tensorflow/tensorflow:latest-py3\n",
    "```\n",
    "\n",
    "This starts an interactive(-it) Jupyter notebook in Docker named 'tf', and binds port 8888 (-p) to host computer. It also links (-v) the current volume ($(pwd)) to starting folder /notebooks inside the Docker. To reach Docker guest system direct browser to localhost:8888. A slight hack to set the timezone to AEST with uniz TZ environmental variable (-e).\n",
    "\n",
    "### Run bash using Docker\n",
    "```bash\n",
    "docker run --rm -it -p 8888:8888 -v $(pwd):/notebooks --name=tf gcr.io/tensorflow/tensorflow:latest-py3 /bin/bash\n",
    "```\n",
    "\n",
    "This starts a bash shell (/bin/bash) inside docker instead of Jupyter server.\n",
    "\n",
    "### Benefits of using Docker\n",
    "- Completely isolates and sandboxes tensorflow environment from host computer\n",
    "- Runs the same on any computer or server for repeatability\n",
    "- Tensorflow can be run on separate server and connected through network\n",
    "- Smaller footprint than Anaconda (tensorflow uses Ubuntu kernel with pip installs)\n",
    "\n",
    "### Cons of using Docker\n",
    "- Docker needs its own resources to run\n",
    "- Although small, there is a memory footprint when host and guest system are the same\n",
    "- Docker gets unstable or crashes outright when out of memory (careful with large networks) -- workaround is to make sure enough memory for all parameters exists before running"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Autoreload magic\n",
    "# makes it possible to work with external files without reloading kernel\n",
    "# See: https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment 1\n",
    "# Share an initializer and value between variables\n",
    "with tf.variable_scope('weights', initializer=tf.truncated_normal_initializer(stddev=0.1)) as weights_scope:\n",
    "    w = tf.get_variable('w', [1])\n",
    "with tf.variable_scope(weights_scope, reuse=True):\n",
    "    w1 = tf.get_variable('w', [1])\n",
    "assert w is w1  # They are indeed the same\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    assert w.eval() == w1.eval()  # And therefor their values too"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment 2\n",
    "# Variable scope affect the variable names\n",
    "# Operators also\n",
    "with tf.variable_scope('cell'):\n",
    "    with tf.name_scope('biases'):\n",
    "        v = tf.get_variable('v', [1])\n",
    "        x = 1.0 + v\n",
    "assert v.name == 'cell/v:0'\n",
    "assert x.op.name == 'cell/biases/add'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "1. Convolution layer to RNN cell\n",
    "    1. Share weights between two RNN cells\n",
    "    2. Stack grid of RNN cells over board of n size\n",
    "2. MDRNN (Multi Dimensional RNN) over board of n size. Based on A. Graves et. al. https://arxiv.org/abs/0705.2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/V:0: (32, 3), parameters: 96\n",
      "biases/by:0: (3,), parameters: 3\n",
      "RNN/MultiRNNCell/Cell0/BasicRNNCell/Linear/Matrix:0: (35, 32), parameters: 1120\n",
      "RNN/MultiRNNCell/Cell0/BasicRNNCell/Linear/Bias:0: (32,), parameters: 32\n",
      "RNN/MultiRNNCell/Cell1/BasicRNNCell/Linear/Matrix:0: (64, 32), parameters: 2048\n",
      "RNN/MultiRNNCell/Cell1/BasicRNNCell/Linear/Bias:0: (32,), parameters: 32\n",
      "total_parameters: 3331\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3\n",
    "# A minimal tensorflow LSTM running example\n",
    "\n",
    "# Supervised learning\n",
    "\n",
    "# Classification Problem:\n",
    "# Given a board of random stones, find a path of connected stones\n",
    "# from one side of the board to the other\n",
    "\n",
    "# Could be turned into a regression problem if the task is to\n",
    "# find the longest path of connected stones or such.\n",
    "# Or possible run unsupervised to find the boards that \n",
    "# connects (find labels)\n",
    "\n",
    "\n",
    "\n",
    "# Experiment \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "# 4 dim array with shape (3, 3, 3, 1)\n",
    "# axis=1: boards\n",
    "# axis=2: rows\n",
    "# axis=3: columns\n",
    "# axis=4: depth\n",
    "# could be scipy.sparse matrix?\n",
    "data = np.array([[[[0], [0], [1]],\n",
    "                  [[0], [1], [0]],\n",
    "                  [[1], [0], [0]]],\n",
    "                 [[[1], [1], [1]],\n",
    "                  [[0], [0], [0]],\n",
    "                  [[1], [1], [1]]],\n",
    "                 [[[0], [1], [0]],\n",
    "                  [[0], [1], [0]],\n",
    "                  [[0], [1], [0]]]])\n",
    "\n",
    "# Label\n",
    "# Single column binary values\n",
    "# 1 dim array with shape (3)\n",
    "# axis=1: connection bool (1 connected, 0 not connected)\n",
    "label = np.array([[0],\n",
    "                  [0],\n",
    "                  [1]])\n",
    "\n",
    "# Initializations\n",
    "# Hyperparameters\n",
    "n = 3  # size of board in data\n",
    "hidden_size = 32  # size of hidden layer of neurons\n",
    "seq_steps = 2  # number of steps to unroll the RNN for\n",
    "num_input = data.shape[0]  # number of boards in data\n",
    "learning_rate = 1e-1\n",
    "batch_size = 1\n",
    "\n",
    "# Experiment 1.A\n",
    "# Create two cells\n",
    "# share weight between them\n",
    "\n",
    "# Model parameters\n",
    "with tf.variable_scope('weights', \n",
    "                       initializer=tf.truncated_normal_initializer(stddev=0.1)):\n",
    "    \n",
    "#     U = tf.get_variable('U', [hidden_size, num_input])  # input to hidden\n",
    "#     W = tf.get_variable('W', [hidden_size, hidden_size])  # hidden to hidden\n",
    "    V = tf.get_variable('V', [hidden_size, num_input])  # hidden to output\n",
    "\n",
    "with tf.variable_scope('biases', initializer=tf.constant_initializer(1)):\n",
    "#     bh = tf.get_variable('bh', [hidden_size])  # hidden biases\n",
    "    by = tf.get_variable('by', [num_input])  # output biases\n",
    "\n",
    "# [batch, time, depth]\n",
    "x = tf.placeholder(tf.float32, [None, seq_steps, num_input])\n",
    "# x = tf.placeholder(tf.float32, [None, n, n, 1])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# conv layer with [1, 1] filter pass info to\n",
    "# RNN layer\n",
    "\n",
    "# 2D cells\n",
    "\n",
    "# MDRNN forward pass\n",
    "# for x1 in range(data.shape[1]):\n",
    "#     for x2 in range(data.shape[2]):\n",
    "\n",
    "\n",
    "# classy inscript class\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.nn.rnn_cell import RNNCell\n",
    "\n",
    "class CustomRNNCell(RNNCell):\n",
    "  \"\"\"The most basic RNN cell.\"\"\"\n",
    "\n",
    "  def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "    if input_size is not None:\n",
    "      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "    self._num_units = num_units\n",
    "    self._activation = activation\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n",
    "      output = self._activation()\n",
    "    return output, output    \n",
    "    \n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "\n",
    "with tf.variable_scope('shared', reuse=True):\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell] * 2)\n",
    "# cell = tf.nn.rnn_cell.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "\n",
    "# Notes on RNN:\n",
    "# tf wants shape [time, batch, depth]\n",
    "# then transpose into [batch, time, depth]\n",
    "# we can split dimentions into several cells\n",
    "\n",
    "# Permute batch and num_steps (see below)\n",
    "# x = tf.transpose(x, [1, 0 ,2])\n",
    "# Reshape into [batch * seq_steps, num_input]\n",
    "# x = tf.reshape(x, [-1, num_input])\n",
    "# Split to get list of (batch, num_input) tensors\n",
    "# x = tf.split(0, seq_steps, x)\n",
    "\n",
    "# Linear activation (Vx + b) of last output\n",
    "# using hidden to output weights\n",
    "pred = tf.matmul(outputs[-1], V) + by\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# count variables\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print('{}: {}, parameters: {}'.format(variable.name, shape, variable_parameters))\n",
    "    total_parameters += variable_parameters\n",
    "print('total_parameters:', total_parameters)\n",
    "\n",
    "\n",
    "# run_metadata = tf.RunMetadata()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "\n",
    "    feed_dict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "#     _ = sess.run(optimizer, \n",
    "#                  options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n",
    "#                  run_metadata=run_metadata\n",
    "#                 feed_dict=feed_dict)\n",
    "    \n",
    "#     param_stats = tf.contrib.tfprof.model_analyzer.print_model_analysis(\n",
    "#         tf.get_default_graph(),\n",
    "#         tfprof_options=tf.contrib.tfprof.model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 4 dim array with dim len 3, 3, 3, 1\n",
    "data = np.array([[[[1], [1], [1]],\n",
    "                  [[1], [1], [1]],\n",
    "                  [[1], [1], [1]]],\n",
    "                 [[[1], [1], [1]],\n",
    "                  [[1], [1], [1]],\n",
    "                  [[1], [1], [1]]],\n",
    "                 [[[1], [1], [1]],\n",
    "                  [[1], [1], [1]],\n",
    "                  [[1], [1], [1]]]])\n",
    "\n",
    "# data = np.array([[[[1], [1], [1]],\n",
    "#                   [[1], [1], [1]],\n",
    "#                   [[1], [1], [1]]]])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4)\n",
      "[[[ 1 13]\n",
      "  [ 2 14]\n",
      "  [ 3 15]\n",
      "  [ 4 16]]\n",
      "\n",
      " [[ 5 17]\n",
      "  [ 6 18]\n",
      "  [ 7 19]\n",
      "  [ 8 20]]\n",
      "\n",
      " [[ 9 21]\n",
      "  [10 22]\n",
      "  [11 23]\n",
      "  [12 24]]]\n",
      "(3, 4, 2)\n",
      "(2, 3, 3)\n",
      "[[[0 0 1]\n",
      "  [0 0 1]]\n",
      "\n",
      " [[0 1 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [1 0 0]]]\n",
      "(3, 2, 3)\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 0]]\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Learning tf.transpose\n",
    "sess = tf.InteractiveSession()\n",
    "# shape=(2, 3, 4)\n",
    "# x = tf.constant([[[1, 2, 3, 4], \n",
    "#                   [5, 6, 7 , 8], \n",
    "#                   [9, 10, 11, 12]], \n",
    "#                  [[13, 14, 15, 16], \n",
    "#                   [17, 18, 19, 20],\n",
    "#                   [21, 22, 23, 24]]])\n",
    "# print(x.get_shape())\n",
    "# a = tf.transpose(x, [1, 2, 0])\n",
    "# print(a.eval())\n",
    "# print(a.get_shape())\n",
    "# # shape=()\n",
    "# y = tf.constant([[[0, 0, 1], \n",
    "#                   [0, 1, 0], \n",
    "#                   [1, 0, 0]], \n",
    "#                  [[0, 0, 1], \n",
    "#                   [0, 1, 0], \n",
    "#                   [1, 0, 0]]])\n",
    "# print(y.get_shape())\n",
    "# b = tf.transpose(y, [1, 0, 2])\n",
    "# print(b.eval())\n",
    "# print(b.get_shape())\n",
    "# b = tf.reshape(b, [-1, 2])\n",
    "# print(b.eval())\n",
    "# b = tf.split(0, 3, b)\n",
    "# print(b[0].eval())\n",
    "\n",
    "# shape (1, 3, 3, 1)\n",
    "x = np.array([[[[1], [2], [3]],\n",
    "               [[4], [5], [6]],\n",
    "               [[7], [8], [9]]]])\n",
    "x.shape\n",
    "x.transpose\n",
    "\n",
    "sess.close()\n",
    "# try these:\n",
    "# Minimal convolutional network\n",
    "# cnn\n",
    "# layered cnn\n",
    "# cnn with shared parameters\n",
    "# Reccurrent nn\n",
    "# rnn with shared parameters\n",
    "# neural calculator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%%bash\n",
    "mkdir -p /config/etc && mv /etc/timezone /config/etc/ && ln -s /config/etc/timezone /etc/\n",
    "echo \"Australia/Brisbane\" > /config/etc/timezone\n",
    "dpkg-reconfigure -f noninteractive tzdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two cells, shared weights\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.one_hot()\n",
    "\n",
    "class CustomRNNCell(RNNCell):\n",
    "  \"\"\"The most basic RNN cell.\"\"\"\n",
    "\n",
    "  def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "    if input_size is not None:\n",
    "      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "    self._num_units = num_units\n",
    "    self._activation = activation\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\"\n",
    "    with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n",
    "      output = self._activation()\n",
    "    return output, output    \n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "\n",
    "with tf.variable_scope('shared', reuse=True):\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell] * 2)\n",
    "# cell = tf.nn.rnn_cell.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "\n",
    "# count variables\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print('{}: {}, parameters: {}'.format(variable.name, shape, variable_parameters))\n",
    "    total_parameters += variable_parameters\n",
    "print('total_parameters:', total_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
