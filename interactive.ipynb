{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[256,32] labels_size=[32,8]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_1, Reshape_2)]]\n\nCaused by op 'SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-ef4a603980d8>\", line 49, in <module>\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(rnn_outputs, y)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1449, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2265, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[256,32] labels_size=[32,8]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_1, Reshape_2)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[256,32] labels_size=[32,8]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_1, Reshape_2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef4a603980d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# model.fit(x, y, n_epoch=1, validation_set=0.1, show_metric=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mtraining_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ef4a603980d8>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(num_epochs, num_steps, verbose)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mxf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[256,32] labels_size=[32,8]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_1, Reshape_2)]]\n\nCaused by op 'SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-ef4a603980d8>\", line 49, in <module>\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(rnn_outputs, y)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1449, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2265, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[256,32] labels_size=[32,8]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_1, Reshape_2)]]\n"
     ]
    }
   ],
   "source": [
    "# import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from board_utils import generate_1d_data\n",
    "from board_utils import build_1d_datasets\n",
    "\n",
    "# interactive notebook\n",
    "# jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "# import ipywidgets as widgets\n",
    "# from ipywidgets import interact\n",
    "\n",
    "# widgets.interact(f, x=10);\n",
    "\n",
    "\n",
    "# def build_graph(\n",
    "#         n_length=8,\n",
    "#         n_hidden=32,\n",
    "#         learning_rate=1e-3):\n",
    "\n",
    "state_size=4  #?\n",
    "n_length=8\n",
    "n_hidden=32\n",
    "learning_rate=1e-3\n",
    "batch_size=32\n",
    "n_classes = 8\n",
    "    \n",
    "data, labels = generate_1d_data(n_length=n_length, one_hot=True)\n",
    "datasets = build_1d_datasets(data, labels)\n",
    "\n",
    "# x = tf.placeholder(tf.float32, [None, n_length, 1])\n",
    "# y = tf.placeholder(tf.float32, [None, n_length])  # one-hot label\n",
    "x = tf.placeholder(tf.int32, [None, n_length])\n",
    "y = tf.placeholder(tf.int32, [None, n_length])\n",
    "init_state = tf.zeros([None, state_size])\n",
    "\n",
    "# RNN inputs\n",
    "x_one_hot = tf.one_hot(x, n_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "#\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# RNN\n",
    "# cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "# init_state = cell.zero_state(batch_size, tf.float32)\n",
    "# rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "outputs, states = tf.nn.seq2seq\n",
    "\n",
    "# FC1\n",
    "# fc1 = tf.reshape(rnn_outputs, [-1, n_length])\n",
    "# y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [n_length, n_hidden])\n",
    "    b = tf.get_variable('b', [n_hidden], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# ?? logits\n",
    "logits = tf.matmul(fc1, W) + b\n",
    "\n",
    "# loss\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(rnn_outputs, y)\n",
    "# loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# regression\n",
    "optimize = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#     return dict(\n",
    "#             x = x,\n",
    "#             y = y,\n",
    "#             init_state = init_state,\n",
    "#             final_state = final_state\n",
    "#             train_step = train_step,\n",
    "#             total_loss = total_loss)\n",
    "\n",
    "def train_network(\n",
    "        num_epochs,\n",
    "        num_steps,\n",
    "        verbose=True):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            xf, yf = datasets.train.next_batch(batch_size)\n",
    "\n",
    "            acc = sess.run(optimize, feed_dict={x: xf, y: yf})\n",
    "            \n",
    "            if step % 50 == 0 and step > 0:\n",
    "                if verbose:\n",
    "                    print('step:{}, acc:{}'.format(step, acc))\n",
    "\n",
    "# Cute code below, but didn't work too well\n",
    "# [samples, timesteps, input dim]\n",
    "# net = learn.\n",
    "# net = tflearn.input_data(shape=[None, n_length, 1])\n",
    "# net = tflearn.lstm(net, n_hidden, return_seq=True)\n",
    "# net = tflearn.lstm(net, n_hidden)\n",
    "# net = tflearn.fully_connected(net, n_length, activation='softmax')\n",
    "# net = tflearn.regression(net)\n",
    "\n",
    "# model = tflearn.DNN(net, tensorboard_verbose=2)\n",
    "# model = tflearn.\n",
    "# model.fit(x, y, n_epoch=1, validation_set=0.1, show_metric=True)\n",
    "\n",
    "training_losses = train_network(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.randint(0, 2, size=[4, 8, 1])\n",
    "\n",
    "# Generate labels from data\n",
    "# The arbitrary problem for the machine is to find the\n",
    "# connection length from left to right.\n",
    "labels = np.zeros([4, 8, 1], dtype=int)\n",
    "for i, board in enumerate(data):\n",
    "\n",
    "    #         if np.sum(board, axis=0) == n:  # quickly get fully connected boards\n",
    "    #             labels[i] = 1\n",
    "    #         else:\n",
    "    #             labels[i] = 0\n",
    "\n",
    "    # Stepwise look for 1's per grid\n",
    "    connection_length = 0\n",
    "    for j, grid in enumerate(board):\n",
    "        if grid == 1:\n",
    "            connection_length += 1\n",
    "        else:\n",
    "            break  # stop looking to save some computation\n",
    "    if connection_length:\n",
    "        labels[i][connection_length - 1] = 1\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class rnn_model(object):\n",
    "    def __init__(self, \n",
    "                 x,\n",
    "                 y=None,\n",
    "                 n_hidden=32,\n",
    "                 batch_size=32):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "#     @property\n",
    "    def inference(self):\n",
    "        '''forward pass up to logits'''\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.n_hidden)\n",
    "        init_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, self.x, dtype=tf.float32)\n",
    "\n",
    "        # FC1\n",
    "        fc1 = tf.reshape(rnn_outputs, [-1, self.n_length])\n",
    "\n",
    "        with tf.variable_scope('softmax'):\n",
    "            W = tf.get_variable('W', [self.n_hidden, self.n_length])\n",
    "            b = tf.get_variable('b', [self.n_length], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # ?? logits\n",
    "        logits = tf.matmul(fc1, W) + b\n",
    "        return logits\n",
    "    \n",
    "#     @property\n",
    "    def loss(self):\n",
    "        y_reshaped = tf.reshape(self.y, [-1])\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(self.logits, y_reshaped)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        return total_loss\n",
    "    \n",
    "#     @property\n",
    "    def optimize(self):\n",
    "        return tf.train.AdamOptimizer(learning_rate).minimize(self.loss, name='optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 126 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 45.8 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.array(np.random.choice(2, size=(4,)))\n",
    "%time np.random.randint(0, 2, size=[1, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'unstack:0' shape=(?, 8) dtype=float32>, <tf.Tensor 'unstack:1' shape=(?, 8) dtype=float32>, <tf.Tensor 'unstack:2' shape=(?, 8) dtype=float32>, <tf.Tensor 'unstack:3' shape=(?, 8) dtype=float32>, <tf.Tensor 'unstack:4' shape=(?, 8) dtype=float32>, <tf.Tensor 'unstack:5' shape=(?, 8) dtype=float32>, <tf.Tensor 'unstack:6' shape=(?, 8) dtype=float32>, <tf.Tensor 'unstack:7' shape=(?, 8) dtype=float32>]\n",
      "step:50, training loss per step:0.0378\n",
      "step:100, training loss per step:0.0151\n",
      "step:150, training loss per step:0.00761\n",
      "step:200, training loss per step:0.00435\n",
      "step:250, training loss per step:0.00276\n",
      "step:300, training loss per step:0.0017\n",
      "step:350, training loss per step:0.0013\n",
      "step:400, training loss per step:0.000902\n",
      "step:450, training loss per step:0.000989\n",
      "step:500, training loss per step:0.000797\n",
      "step:550, training loss per step:0.000568\n",
      "step:600, training loss per step:0.000545\n",
      "step:650, training loss per step:0.000462\n",
      "step:700, training loss per step:0.000422\n",
      "step:750, training loss per step:0.0004\n",
      "step:800, training loss per step:0.000315\n",
      "step:850, training loss per step:0.000255\n",
      "step:900, training loss per step:0.000282\n",
      "step:950, training loss per step:0.000193\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from board_utils import generate_1d_data\n",
    "from board_utils import build_1d_datasets\n",
    "\n",
    "def reset_graph():\n",
    "    '''\n",
    "    reset graph so we can run it multiple times without \n",
    "    duplicating variables (causing nasty errors)\n",
    "    '''\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()  # Close any open session\n",
    "    tf.reset_default_graph()  # clear graph stack\n",
    "\n",
    "def build_graph(state_size=4,  #?\n",
    "                n_length=8,\n",
    "                num_hidden=32,\n",
    "                learning_rate=1e-3,\n",
    "                batch_size=32,\n",
    "                num_classes=8,\n",
    "                num_steps=8):\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    # placeholders\n",
    "    #\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [None, n_length])\n",
    "    y = tf.placeholder(tf.int32, [None, n_length])\n",
    "\n",
    "    # Inputs\n",
    "    #\n",
    "    \n",
    "    embeddings = tf.get_variable('embedding_matrix', [n_length, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)  # get correct shape of input\n",
    "    print(rnn_inputs)\n",
    "\n",
    "    # RNN\n",
    "    #\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Add rnn_cells to graph\n",
    "    # dynamic_rnn wants input shape [batch, time, state_size]\n",
    "    # alternatively use nested tensors of [[batch, time]]\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "    # Prediction loss and optimize\n",
    "    #\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    # reshape rnn_outputs so a single matmul is possible\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "    \n",
    "#     logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "#     predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "#     y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "#     losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_reshaped, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    optimize = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        optimize = optimize)\n",
    "    \n",
    "def train_network(g,\n",
    "                  num_epochs,\n",
    "                  num_steps,\n",
    "                  batch_size=32,\n",
    "                  verbose=True):\n",
    "\n",
    "    data, labels = generate_1d_data(8, one_hot=True)\n",
    "    datasets = build_1d_datasets(data, labels)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            xf, yf = datasets.train.next_batch(batch_size)\n",
    "\n",
    "            _, training_loss = sess.run([g['optimize'], g['total_loss']], \n",
    "                         feed_dict={g['x']: xf, g['y']: yf})\n",
    "            \n",
    "            total_loss += training_loss\n",
    "            if step % 50 == 0 and step > 0:\n",
    "                if verbose:\n",
    "                    print('step:{}, training loss per step:{:.3}'.format(step, training_loss / step))\n",
    "\n",
    "\n",
    "g = build_graph()\n",
    "train_network(g, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"RNN/transpose:0\", shape=(32, 5, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?,) dtype=int32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Debugging input shapes\n",
    "'''\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, 8])\n",
    "y = tf.placeholder(tf.int32, [None, 8])\n",
    "embeddings = tf.get_variable('emb_matrix', [8, 4])\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "rnn_inputs\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(4)\n",
    "init_state = cell.zero_state(32, tf.float32)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "print(rnn_outputs)\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, 4])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "rnn_outputs\n",
    "y_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
